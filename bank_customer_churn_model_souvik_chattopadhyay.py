# -*- coding: utf-8 -*-
"""Bank Customer Churn Model_Souvik Chattopadhyay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4OImbVeLnE5y5R2C4UDmFHfx5QMwuy4
"""

# Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Import Dataset
df = pd.read_csv(' https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

df.head()

df.info()

df.duplicated('CustomerId').sum()

df = df.set_index('CustomerId')

df.info()

# Data Encoding

df['Geography'].value_counts()

df.replace({'Geography': {'France':2, 'Germany':1, 'Spain':0}}, inplace=True)

df['Gender'].value_counts()

df.replace({'Gender':{'Male':0, 'Female':1}}, inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num of Products':{1:0, 2:1, 3:1, 4:1}}, inplace = True)

df['Has Credit Card'].value_counts()

df.loc[(df['Balance']==0), 'Churn'].value_counts()

df['Zero Balance'] = np.where(df['Balance']>0,1,0)

df['Zero Balance'].hist()

df.groupby(['Churn','Geography']).count()

# Define Label and Features

df.columns

X = df.drop(['Surname','Churn'], axis = 1)

y = df['Churn']

X.shape, y.shape

# Oversampling

df['Churn'].value_counts()

sns.countplot(x='Churn', data = df);

X.shape, y.shape

# Undersampling

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=2529)

X_rus, y_rus = rus.fit_resample(X,y)

X_rus.shape, y_rus.shape, X.shape, y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind = 'hist')

# Random Over Sampling // we are not losing any information which was available in our initial dataset rather than others

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=2529)

X_ros, y_ros = ros.fit_resample(X,y)

X_ros.shape, y_ros.shape, X.shape, y.shape

y.value_counts()

y_ros.value_counts()

y_ros.plot(kind = 'hist')

# Step 4: Train Test Split
from sklearn.model_selection import train_test_split

# Split Original Data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=2527)

# Split Random Under Sampled Data
X_train_rus, X_test_rus, y_train_rus, y_test_rus = train_test_split(X_rus, y_rus, test_size=0.3, random_state=2527)

# Split Random Over Sampled Data
X_train_rus, X_test_rus, y_train_rus, y_test_rus = train_test_split(X_rus, y_rus, test_size=0.3, random_state=2527)

# Standardize Features

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

# Standardize Original Data

X_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

X_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

# Standardize Random Under Sampled Data

X_train_rus[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']] = sc.fit_transform(X_train_rus[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

X_test_rus[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']] = sc.fit_transform(X_test_rus[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Standardize Random Over Sampled Data

X_ros[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']] = sc.fit_transform(X_ros[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

X_ros[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']] = sc.transform(X_ros[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Support Vector Machine Classifier

from sklearn.svm import SVC

svc = SVC()

svc.fit(X_train, y_train)

y_pred = svc.predict(X_test)

# Model Accuracy

from sklearn.metrics import confusion_matrix, classification_report

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

# WAY -> 2

from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.1,1,10],
              'gamma': [1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight': ['balanced']}

grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid.fit(X_train,y_train)

print(grid.best_estimator_)

grid_predictions = grid.predict(X_test)

confusion_matrix(y_test,grid_predictions)

print(classification_report(y_test,grid_predictions))

# Model with Random under Sampling

svc_rus = SVC()

svc_rus.fit(X_train_rus,y_train_rus)

y_pred_rus = svc_rus.predict(X_test_rus)

# Model Accuracy

confusion_matrix(y_test_rus, y_pred_rus)

print(classification_report(y_test_rus,y_pred_rus))

# Model with Random Over Sampling

# Import necessary libraries
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Example dataset creation (Replace this with your actual data loading code)
# Assuming you have a dataset with these features and a target variable
data = {
    'CreditScore': [600, 700, 800, 900, 650, 720, 810, 950],
    'Age': [25, 45, 35, 50, 22, 40, 60, 70],
    'Tenure': [1, 2, 3, 4, 5, 6, 7, 8],
    'Balance': [10000, 20000, 15000, 30000, 12000, 25000, 18000, 32000],
    'Estimated Salary': [50000, 60000, 55000, 70000, 52000, 61000, 58000, 75000],
    'Target': [0, 1, 0, 1, 0, 1, 0, 1]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features and target variable
X = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']]
y = df['Target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define and fit the scaler on the training data
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Perform random oversampling
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled, y_train)

# Initialize the SVM model
svc_ros = SVC()

# Train the model on the oversampled data
svc_ros.fit(X_train_ros, y_train_ros)

# Standardize the test data using the same scaler fitted on the training data
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Make predictions on the original test data
grid_predictions_ros = svc_ros.predict(X_test_scaled)

# Print the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, grid_predictions_ros))

# Import necessary libraries
from imblearn.over_sampling import RandomOverSampler
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Assuming your original training and test data are defined somewhere above
# X_train = your training feature dataframe
# y_train = your training labels
# X_test = your test feature dataframe
# y_test = your test labels

# Define and fit the scaler on the training data
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Perform random oversampling on the training data
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled, y_train)

# Scale the test data using the same scaler fitted on the training data
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Initialize the SVM model
svc_ros = SVC()

# Train the model on the oversampled data
svc_ros.fit(X_train_ros, y_train_ros)

# Predict on the scaled test data
y_pred_ros = svc_ros.predict(X_test_scaled)

# Model Accuracy

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# Assuming your original dataset is defined
# X = your feature dataframe
# y = your labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define and fit the scaler on the training data
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Perform random oversampling on the training data
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled, y_train)

# Initialize and train the SVM model on the oversampled training data
svc_ros = SVC()
svc_ros.fit(X_train_ros, y_train_ros)

# Predict on the scaled test data
y_pred_ros = svc_ros.predict(X_test_scaled)

# Evaluate the model using a confusion matrix
cm = confusion_matrix(y_test, y_pred_ros)
print(cm)

# Import necessary libraries
from imblearn.over_sampling import RandomOverSampler
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Example data preparation
# Assuming X and y are your features and labels
# Replace with actual data loading/preparation code
# X, y = ...

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the training data
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Standardize the test data using the same scaler
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Perform random oversampling on the training data
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled, y_train)

# Initialize the SVM model
svc_ros = SVC()

# Train the model on the oversampled training data
svc_ros.fit(X_train_ros, y_train_ros)

# Predict on the test data
y_pred_ros = svc_ros.predict(X_test_scaled)

# Print the classification report
print(classification_report(y_test, y_pred_ros))

# Hyperparameter Tunning

param_grid = {'C': [0.1,1,10],
              'gamma': [1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight': ['balanced']}

grid_ros = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid_ros.fit(X_train_ros,y_train_ros)

print(grid_ros.best_estimator_)

# Assuming X_test is your original test data
# Standardize the test data using the same scaler fitted on the training data
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Make predictions using the grid search model
grid_predictions_ros = grid_ros.predict(X_test_scaled)

# Import necessary libraries
from sklearn.metrics import confusion_matrix

# Assuming X_test and y_test are your original test data
# Scale the test data using the same scaler fitted on the training data
X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

# Make predictions using the trained model
grid_predictions_ros = svc_ros.predict(X_test_scaled)

# Compute the confusion matrix
confusion_matrix(y_test, grid_predictions_ros)

from sklearn.metrics import classification_report

X_test_scaled = sc.transform(X_test[['CreditScore', 'Age', 'Tenure', 'Balance', 'Estimated Salary']])

grid_predictions_ros = svc_ros.predict(X_test_scaled)

print(classification_report(y_test, grid_predictions_ros))

